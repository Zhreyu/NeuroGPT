import torch
from torch.utils.data import DataLoader
import logging
import argparse
from src.encoder.conformer_braindecode import EEGConformer
from src.embedder.base import BaseEmbedder
from src.decoder.gpt import GPT2Model as GPTDecoder
from src.model import Model
from src.batcher.downstream_dataset import EEGDatasetCls

#!/usr/bin/env python3 
import torch
from typing import Dict
import warnings
from safetensors.torch import load_model
import os 
class Model(torch.nn.Module):
    """
    Create Model object from embedder, decoder,
    and unembedder (if not None).

    Args
    ----
    embedder: src.embedder.make_embedder
        Instance of embedder class.
    decoder: src.decoder.make_decoder
        Instance of decoder class.
    unembedder: src.unembedder.make_unembedder
        Instance of unembedder class.
        Only added to model if not None.

    Methods
    ----
    forward(batch: Dict[str, torch.tensor])
        Forward pass of model.
    prep_batch(batch: Dict[str, torch.tensor])
        Prepare batch for forward pass.
    compute_loss(batch: Dict[str, torch.tensor])
        Compute training loss.
    from_pretrained(pretrained_path: str)
        Load pretrained model from pretrained_path.
        Needs to point to pytorch_model.bin file 
    """
    def __init__(
        self,
        encoder: torch.nn.Module,
        embedder: torch.nn.Module,
        decoder: torch.nn.Module,
        unembedder: torch.nn.Module = None
        ) -> torch.nn.Module:
        
        super().__init__()
        self.name = f'Embedder-{embedder.name}_Decoder-{decoder.name}'
        self.encoder = encoder
        self.embedder = embedder
        self.decoder = decoder
        self.unembedder = unembedder
        self.is_decoding_mode = False
        self.ft_only_encoder = False

    def from_pretrained(self, pretrained_path: str) -> None:
        """Load pretrained model from a .pt file or a .safetensors file."""
        print(f'Loading pretrained model from {pretrained_path}')
        
        file_ext = os.path.splitext(pretrained_path)[1]
        
        if file_ext == '.pt' or file_ext == '.bin':
            device = 'cuda' if next(self.parameters()).is_cuda else 'cpu'
            pretrained = torch.load(pretrained_path, map_location=torch.device(device))
        elif file_ext == '.safetensors':
            load_model(self, pretrained_path,strict=False)  # Corrected to pass the instance itself
        else:
            raise ValueError("Unsupported file format. Expected '.pt' or '.safetensors'.")

        print('Pretrained model loaded successfully.')

        
        
    def switch_ft_mode(self, ft_encoder_only=False):
        self.ft_only_encoder = ft_encoder_only

    def switch_decoding_mode(
        self,
        is_decoding_mode: bool = False,
        num_decoding_classes: int = None
        ) -> None:
        """Switch model to decoding model or back to training mode.
        Necessary to adapt pre-trained models to downstream
        decoding tasks.
        
        Args
        ----
        is_decoding_mode: bool
            Whether to switch to decoding mode or not.
        num_decoding_classes: int
            Number of classes to use for decoding.    
        """
        self.is_decoding_mode = is_decoding_mode
        
        self.embedder.switch_decoding_mode(is_decoding_mode=is_decoding_mode)
        self.decoder.switch_decoding_mode(
            is_decoding_mode=is_decoding_mode,
            num_decoding_classes=num_decoding_classes
        )

    def compute_loss(
        self,
        batch: Dict[str, torch.tensor],
        return_outputs: bool = False
        ) -> Dict[str, torch.tensor]:
        """
        Compute training loss, based on 
        embedder's training-style.

        Args
        ----
        batch: Dict[str, torch.tensor]
            Input batch (as generated by src.batcher)
        return_outputs: bool
            Whether to return outputs of forward pass
            or not. If False, only loss is returned.

        Returns
        ----
        losses: Dict[str, torch.tensor]
            Training losses.
        outputs: torch.tensor
            Outputs of forward pass.
        """
        (outputs, batch) = self.forward(
            batch=batch,
            return_batch=True
        )
        losses = self.embedder.loss(
            batch=batch,
            outputs=outputs
        )

        return (losses, outputs) if return_outputs else losses

    def prep_batch(
        self,
        batch: Dict[str, torch.tensor]
        ) -> Dict[str, torch.tensor]:
        """Prepare input batch for forward pass.
        Calls src.embedder.prep_batch.
        
        Args
        ----
        batch: Dict[str, torch.tensor]
            Input batch (as generated by src.batcher)
        """
        return self.embedder.prep_batch(batch=dict(batch))

    def forward(
        self,
        batch: Dict[str, torch.tensor],
        prep_batch: bool = True,
        return_batch: bool = False
        ) -> torch.tensor:
        """
        Forward pass of model.
        
        Args
        ----
        batch: Dict[str, torch.tensor]
            Input batch (as generated by src.batcher)
        prep_batch: bool
            Whether to prep batch for forward pass
            by calling self.embedder.prep_batch
        return_batch: bool
            Whether to return batch after forward pass
            or not. If False, only outputs of forward pass
            are returned.

        Returns
        ----
        outputs: torch.tensor
            Outputs of forward pass.
        batch: Dict[str, torch.tensor]
            Input batch (as returned by prep_batch, 
            if prep_batch is True)
        """
        
        if self.encoder is not None:
            #before prep_batch masking and things, we need to first let the splitted chunks of raw input through the encoder
            features = self.encoder(batch['inputs'])
            #attempt for trying fine-tune only the encoder, but the encoder cannot combine information across chunks.
            if self.is_decoding_mode and self.ft_only_encoder:
                outputs={'outputs': features, 'decoding_logits': features}
                return (outputs, batch) if return_batch else outputs

            b, f1, f2 = features.size()
            nchunks = batch['inputs'].size()[1]
            batch['inputs'] = features.view(b//nchunks, nchunks, f1*f2)
        
        if prep_batch:
            if len(batch['inputs'].size()) > 3:
                bsize, chunk, chann, time = batch['inputs'].size() 
                batch['inputs'] = batch['inputs'].view(bsize, chunk, chann*time)
            batch = self.prep_batch(batch=batch)
            # batch['inputs_embeds'] = batch['inputs_embeds'].view(bsize, chunk, chann, time)
            # print("preparing batch")
        else:
            assert 'inputs_embeds' in batch, 'inputs_embeds not in batch'

        # pdb.set_trace()
        batch['inputs_embeds'] = self.embedder(batch=batch)
        outputs = self.decoder(batch=batch)
        
        if self.unembedder is not None and not self.is_decoding_mode:
            outputs['outputs'] = self.unembedder(inputs=outputs['outputs'])['outputs']

        return (outputs, batch) if return_batch else outputs

def load_data(data_dir: str, batch_size: int, shuffle: bool = False) -> DataLoader:
    """
    Load data using the existing DataLoader setup.
    
    Args:
        data_dir (str): Directory path for the dataset.
        batch_size (int): Number of samples per batch.
        shuffle (bool): Whether to shuffle the data.
    
    Returns:
        DataLoader: DataLoader object for the dataset.
    """
    # Initialize the dataset
    dataset = EEGDatasetCls(data_dir)
    
    # Create DataLoader
    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)
    
    return data_loader

def perform_predictions(model: torch.nn.Module, data_loader: DataLoader, device: torch.device) -> list:
    """
    Perform predictions on the loaded data.
    
    Args:
        model (torch.nn.Module): The loaded model for predictions.
        data_loader (DataLoader): DataLoader object containing the data.
        device (torch.device): Device to perform computations on (CPU or GPU).
    
    Returns:
        list: List of predictions for each batch.
    """
    model.to(device)
    predictions = []
    
    with torch.no_grad():
        for batch in data_loader:
            inputs = batch['inputs'].to(device)
            outputs = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            predictions.extend(predicted.cpu().numpy())
    
    return predictions

def save_predictions(predictions: list, output_path: str):
    """
    Save predictions to a specified file.
    
    Args:
        predictions (list): List of predictions.
        output_path (str): Path to save the predictions file.
    """
    with open(output_path, 'w') as f:
        for pred in predictions:
            f.write(f"{pred}\n")

def main(args):
    # Load the trained model
    model = from_pretrained(args.model_path)
    
    # Load the data
    data_loader = load_data(args.data_dir, args.batch_size, shuffle=args.shuffle)
    
    # Set device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # Perform predictions
    predictions = perform_predictions(model, data_loader, device)
    
    # Save predictions
    save_predictions(predictions, args.output_path)
    
    logger.info(f"Predictions saved to {args.output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Model Prediction Script")
    parser.add_argument('--model_path', type=str, required=True, help="Path to the pre-trained model file")
    parser.add_argument('--data_dir', type=str, required=True, help="Directory path for the dataset")
    parser.add_argument('--batch_size', type=int, default=32, help="Number of samples per batch")
    parser.add_argument('--shuffle', type=bool, default=False, help="Whether to shuffle the data")
    parser.add_argument('--output_path', type=str, required=True, help="Path to save the predictions file")
    
    args = parser.parse_args()
    main(args)
